## Convergence testing

Here's an example sent to me by KT (who shall otherwise remain anonymous).
He wanted to know why he was getting singular fits (i.e., one of the variance
components is zero):

```{r pkgs,message=FALSE}
library(lattice)
library(lme4)
library(blme)
library(reshape2)
library(ggplot2); theme_set(theme_bw())
library(gridExtra)  ## for grid.arrange
library(bbmle) ## for slice2D; requires *latest* r-forge version (r121)
source("../R/allFit.R")
```

Load data:

```{r getdat}
dList <- load("../data/ktdata.RData")
```

First, do a general check of the data.
In the spaghetti plot (lines for all individuals (`pid`)
plotted on the same panel): don't see much pattern other than (1) general
increasing trend; (2) quantized response values (`table(dth$Estimate)`
or `unique(dth$Estimate)` also show this); (3) skewed residuals.


```{r spaghett1,warning=FALSE}
(p0 <- ggplot(dth,aes(Actual,Estimate))+
 geom_point()+
 ## lines by pid; lighten lines slightly
 geom_line(aes(group=factor(pid)),alpha=0.4)+  
 stat_summary(fun.y=mean,geom="line",colour="red",lwd=1.5))  ## add mean line
```

Check all unique values of `Estimate`:
```{r usort}
sort(unique(dth$Estimate))
```

Emphasize distribution: `stat_sum` shows the number of overlapping points
at a location.
```{r sumplot}
ggplot(dth,aes(x=Actual,y=Estimate))+
    stat_sum(aes(size=..n..),alpha=0.5)+
    scale_size_continuous(range=c(2,8))
```

We can fit this model OK, but we get a correlation of 100% between
the slope and intercept:
```{r fit1,message=FALSE}
(f1 <- lmer(Estimate~Actual+(Actual|pid),data=dth,REML=FALSE))
```    

A utility function for extracting just the correlation:
```{r getcor}
getcor <- function(object) attr(VarCorr(object)[[1]],"correlation")[2,1]
getcor(f1)
```

The Q-Q plot of the residuals is pretty horrible:
```{r}
qqmath(f1)
```

It occurred to me that one way to fix the problem was to
log-transform the data (because there are zero values, I'm just
going to use `log(0.5+x)` as the response -- this is a bit
sloppy/*ad hoc*; `log(1+x)` may look
even better ...) This helps a great
deal with the fit (it's no longer singular), and with the Q-Q plot.

Plot the data on this scale: add individual regression lines
```{r sumplot2,message=FALSE}
ggplot(dth,aes(x=Actual,y=log(0.5+Estimate)))+
    stat_sum(aes(size=..n..),alpha=0.5)+
    scale_size_continuous(range=c(2,8))+
    geom_smooth(method="lm",
                aes(group=pid),
                se=FALSE,
                ## 'alpha' refers to the transparency of the
                ##  confidence ribbon (which we have suppressed
                ##  via se=FALSE); use adjustcolor() (from base R)
                ##  to make the line color partly transparent
                colour=adjustcolor("gray",alpha=0.3))+
    geom_smooth(method="lm",alpha=0.6) ## overall linear regression line
```
The message (`Don't know how to automatically pick scale ...`) is
a false positive caused by using labelled variables (from the `Hmisc` package)

```{r fit1L}
f1L <- lmer(log10(0.5+Estimate)~Actual+(Actual|pid),data=dth,REML=FALSE)
(vc1L <- VarCorr(f1L))
```

```{r fit1L_invis,echo=FALSE}
vcratio <- sqrt(vc1L[[1]][2,2]/vc1L[[1]][1,1])
```
The intercept variance is pretty small (`r round(100*vcratio)`% of the 
slope variance, but bounded.

The Q-Q plot is much better (although still slightly left-skewed):

```{r qq,fig.width=10}
grid.arrange(qqmath(f1,main="Original"),qqmath(f1L,main="Log scale"),nrow=1)
```

Seems to help a bit with the residual vs. fitted plot:
```{r diag,fig.width=10}
grid.arrange(plot(f1,type=c("p","smooth"),main="Original"),
             plot(f1L,type=c("p","smooth"),main="Log scale"),nrow=1)
```

(Optionally) double-check that the same thing happens with `lme4.0`,
the old stable version (use `::` to use the `lme4.0` functions
without actually loading the `lme4.0`
package, which will conflict with the `lme4` package ...)

```{r otherfits}
f2 <- lme4.0::lmer(Estimate~Actual+(Actual|pid),data=dth,REML=FALSE)
attr(lme4.0::VarCorr(f2)[[1]],"correlation")[2,1]
```

Unfortunately, KT reported that for this problem it was important
to fit on the original scale rather than the log scale -- we
will double-check with all available optimizers to see if they
all get to the same answer ...

Fitting with all possible optimizers:
```{r allfits,results="hide",warning=FALSE,cache=TRUE}
aa <- allFit(f1)
ss <- summary.allfit(aa)
```

Results: warning for L-BFGS-B (which does indeed fail); all other
results are nearly identical (`nlminb` gives cor *slightly* <1) (only
sd/cor estimates shown; log-likelihoods and fixed effect estimates
are consistent with this picture)
(Facet and axis tick labels below are slightly wonky/need tweaking.)

```{r sdcor1,message=FALSE,warning=FALSE,fig.width=10}
names(which(!sapply(ss$msgs,is.null)))
(g0 <- ggplot(melt(ss$sdcor),aes(Var2,value,colour=Var1))+
    geom_point(position=position_dodge(width=0.5))+
    facet_wrap(~Var2,scale="free"))
```

## blmer
L-BFGS-B fails completely (unusual error, missing value pops up in internal boundary test -- should investigate)
none of the other cases gives convergence warnings, which is slightly alarming since both versions of BOBYQA (`minqa` and `nloptr`) clearly fail, with NLL 50-100 log-likelihood units worse than the other fits ... I think the problem here is actually something in `blmer` where the warnings fail to get stored properly ...


```{r blmefit,cache=TRUE}
## hack to refit lmer as glmer
ff <- getCall(f1)
ff[[1]] <- quote(blmer)
f3 <- eval(ff)
aa3 <- allFit(f3)
ss3 <- summary.allfit(aa3)
```

Despite the failure of `blme` to *store* the warnings, we can
see from the output that the warnings do correspond to the
incorrect results (i.e., both BOBYQA implementations).
It's interesting that BOBYQA fails here, since in most
other cases we have found it to be the *most* robust algorithm ...

```{r blmeplot,message=FALSE,fig.width=10}
g0 %+% melt(cbind(ss3$sdcor,NLL=-2*(ss3$ll-max(ss3$ll))))
```

## Visualizing likelihood surface

```{r slicecalc,cache=TRUE,results="hide"}
## set up ranges for each parameter
(rmat <- matrix(c(0,1,
                 -0.2,0.2,
                 0,1),byrow=TRUE,ncol=2,
               dimnames=list(paste0("theta",1:3),c("lwr","upr"))))
## extract deviance function
dd1 <- update(f1,devFunOnly=TRUE)
## extract theta parameters from the fit
pp1 <- getME(f1,"theta")
## compute 2D likelihood 'slices' for each pair of parameters
##  (*not* profiles, which would be much more expensive)
s2D.1 <- bbmle:::slice2D(pp1,dd1,
                       tranges=rmat)
## do the same for the blme fit:
dd3 <- update(f3,devFunOnly=TRUE)
pp3 <- getME(aa3[[2]],"theta")  ## use an optimizer that actually worked
s2D.3 <- bbmle:::slice2D(pp3,dd3,
                       tranges=rmat)
```

Left plot, `lmer`; right, `blmer`.

```{r sploms,fig.width=10}
grid.arrange(splom(s2D.1,main="lmer"),splom(s2D.3,main="blmer"),nrow=1)
```

It still looks like the `blmer` parameters are on the boundary, 
but if we zoom in a bit we can see that they're not:

```{r slicecalc2,cache=TRUE,results="hide"}
rmat2 <- matrix(c(0,1,-0.1,0.1,0,0.1),byrow=TRUE,ncol=2,
               dimnames=list(paste0("theta",1:3),c("lwr","upr")))
s2D.1b <- bbmle:::slice2D(pp1,dd1,
                       tranges=rmat2)
s2D.3b <- bbmle:::slice2D(pp3,dd3,
                       tranges=rmat2)

```{r sploms2,fig.width=10}
grid.arrange(splom(s2D.1b,main="lmer"),splom(s2D.3b,main="blmer"),nrow=1)
```

We can plot with contours, which is pretty ...
```{r sploms3}
splom(s2D.3b,contour=TRUE)
```

But the weird parts of these contours are nowhere near the relevant
confidence intervals (this should be built into to the `slice` plotting
method instead of having to be done by hand, but for now ...)
```{r}
ss0 <- s2D.3b$slices[[1]][[2]]  ## pull out (theta1,theta2) surface
ss1 <- with(ss0,list(x=unique(x),y=unique(y),z=matrix(z,nrow=31)))
with(ss1,contour(x,y,z-min(z),col="gray"))
with(ss1,contour(x,y,z-min(z),
                 levels=qchisq(c(0.9,0.95),2),
                 col=c(2,4),labels=c("90","95"),
                 add=TRUE))
```

Visualizations could use a lot of work (scale bar and/or
contours; allow for more points, e.g. visualize where non-converging
fits ended up ...)

### Gauss with log link?

This is a nice idea but fails ... if we don't
tweak the LHS we get `NaN` values immediately
(these *should* provoke a more useful warning/error message!);
if we use an offset of 0.1 then we get a PWRSS failure; with
an offset of 0.5 we get another singular fit.
```{r}
(f4 <- glmer(Estimate+0.5~Actual+(Actual|pid),data=dth,
            family=gaussian(link="log")))
```

Stuff left out (so far):

* other data sets (`dord`, `dorltc`) (could follow similar strategies)
